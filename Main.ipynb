{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from resnet import *\n",
    "from cifar import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 5\n",
    "epoch = 100\n",
    "image_shape = [32, 32, 3]\n",
    "train_batch_size = 128\n",
    "test_batch_size = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, step 390, learning rate 0.009999999776482582\n",
      "    train loss 1.6755323196068788, train error 0.6243790064102563\n",
      "    test loss 1.51058778911829, test_error 0.5453999977558851\n",
      "Training epoch 1, step 780, learning rate 0.10000000149011612\n",
      "    train loss 1.242653550245823, train error 0.4510416666666667\n",
      "    test loss 1.7205868989229203, test_error 0.5357999995350837\n",
      "Training epoch 2, step 1170, learning rate 0.10000000149011612\n",
      "    train loss 0.8737504081848341, train error 0.3101963141025641\n",
      "    test loss 1.0621743306517601, test_error 0.3560000009834766\n",
      "Training epoch 3, step 1560, learning rate 0.10000000149011612\n",
      "    train loss 0.685145598802811, train error 0.23778044871794868\n",
      "    test loss 0.9372310861945152, test_error 0.2964999966323376\n",
      "Training epoch 4, step 1950, learning rate 0.10000000149011612\n",
      "    train loss 0.5845388007469666, train error 0.20086137820512817\n",
      "    test loss 0.7539100520312786, test_error 0.24909999892115597\n",
      "Training epoch 5, step 2340, learning rate 0.10000000149011612\n",
      "    train loss 0.5174463078761712, train error 0.17854567307692304\n",
      "    test loss 0.6415861312299966, test_error 0.21439999938011167\n",
      "Training epoch 6, step 2730, learning rate 0.10000000149011612\n",
      "    train loss 0.468751904521233, train error 0.16073717948717947\n",
      "    test loss 0.5846715711057187, test_error 0.19470000192523007\n",
      "Training epoch 7, step 3120, learning rate 0.10000000149011612\n",
      "    train loss 0.4297776153072333, train error 0.1466947115384616\n",
      "    test loss 0.5743131004273891, test_error 0.1897999987006187\n",
      "Training epoch 8, step 3510, learning rate 0.10000000149011612\n",
      "    train loss 0.3947185264566006, train error 0.13435496794871793\n",
      "    test loss 0.5226643502712249, test_error 0.17459999844431873\n",
      "Training epoch 9, step 3900, learning rate 0.10000000149011612\n",
      "    train loss 0.37057020029960536, train error 0.1253405448717949\n",
      "    test loss 0.501189985871315, test_error 0.16999999731779103\n",
      "Training epoch 10, step 4290, learning rate 0.10000000149011612\n",
      "    train loss 0.345308293784276, train error 0.11740785256410258\n",
      "    test loss 0.4929979393258691, test_error 0.16069999858736994\n",
      "Training epoch 11, step 4680, learning rate 0.10000000149011612\n",
      "    train loss 0.3234795841269004, train error 0.10849358974358969\n",
      "    test loss 0.5883966770023108, test_error 0.18539999872446056\n",
      "Training epoch 12, step 5070, learning rate 0.10000000149011612\n",
      "    train loss 0.3038620474246832, train error 0.1019230769230769\n",
      "    test loss 0.4866388265043497, test_error 0.15709999874234204\n",
      "Training epoch 13, step 5460, learning rate 0.10000000149011612\n",
      "    train loss 0.288405159727121, train error 0.09717548076923077\n",
      "    test loss 0.4907652523368597, test_error 0.15959999933838842\n",
      "Training epoch 14, step 5850, learning rate 0.10000000149011612\n",
      "    train loss 0.2737200574232982, train error 0.09242788461538465\n",
      "    test loss 0.5045559290796519, test_error 0.15440000072121618\n",
      "Training epoch 15, step 6240, learning rate 0.10000000149011612\n",
      "    train loss 0.2581985995937616, train error 0.0855769230769231\n",
      "    test loss 0.4752490971237421, test_error 0.14869999960064884\n",
      "Training epoch 16, step 6630, learning rate 0.10000000149011612\n",
      "    train loss 0.24506808874698785, train error 0.08143028846153844\n",
      "    test loss 0.45707728546112775, test_error 0.14460000023245811\n",
      "Training epoch 17, step 7020, learning rate 0.10000000149011612\n",
      "    train loss 0.23677907866927295, train error 0.07984775641025643\n",
      "    test loss 0.4561734776943922, test_error 0.14249999672174452\n",
      "Training epoch 18, step 7410, learning rate 0.10000000149011612\n",
      "    train loss 0.2234864629423007, train error 0.07459935897435899\n",
      "    test loss 0.45048036258667706, test_error 0.1394999995827675\n",
      "Training epoch 19, step 7800, learning rate 0.10000000149011612\n",
      "    train loss 0.20958872926540864, train error 0.06887019230769231\n",
      "    test loss 0.4695254048332572, test_error 0.1415999956429005\n",
      "Training epoch 20, step 8190, learning rate 0.10000000149011612\n",
      "    train loss 0.20316044115103207, train error 0.06632612179487174\n",
      "    test loss 0.4138853676617146, test_error 0.13230000138282771\n",
      "Training epoch 21, step 8580, learning rate 0.10000000149011612\n",
      "    train loss 0.19228379871600715, train error 0.06332131410256414\n",
      "    test loss 0.46016706731170415, test_error 0.13499999940395357\n",
      "Training epoch 22, step 8970, learning rate 0.10000000149011612\n",
      "    train loss 0.1859349940258723, train error 0.06149839743589747\n",
      "    test loss 0.4748202493414283, test_error 0.13859999924898148\n",
      "Training epoch 23, step 9360, learning rate 0.10000000149011612\n",
      "    train loss 0.1747667021667346, train error 0.05671073717948716\n",
      "    test loss 0.4176105072721839, test_error 0.12539999634027477\n",
      "Training epoch 24, step 9750, learning rate 0.10000000149011612\n",
      "    train loss 0.16938152464154438, train error 0.056510416666666674\n",
      "    test loss 0.5345833536237479, test_error 0.1500999972224235\n",
      "Training epoch 25, step 10140, learning rate 0.10000000149011612\n",
      "    train loss 0.15951531240955377, train error 0.051542467948717974\n",
      "    test loss 0.45356850866228343, test_error 0.12869999408721922\n",
      "Training epoch 26, step 10530, learning rate 0.10000000149011612\n",
      "    train loss 0.15578840479063683, train error 0.050340544871794846\n",
      "    test loss 0.5232213927432895, test_error 0.14259999841451643\n",
      "Training epoch 27, step 10920, learning rate 0.10000000149011612\n",
      "    train loss 0.14721211311526788, train error 0.04779647435897438\n",
      "    test loss 0.46019284389913084, test_error 0.13409999758005142\n",
      "Training epoch 28, step 11310, learning rate 0.10000000149011612\n",
      "    train loss 0.14111311133855428, train error 0.04469150641025643\n",
      "    test loss 0.49080816488713025, test_error 0.13419999629259105\n",
      "Training epoch 29, step 11700, learning rate 0.10000000149011612\n",
      "    train loss 0.13666065245484693, train error 0.043910256410256365\n",
      "    test loss 0.4253996221348643, test_error 0.12260000258684156\n",
      "Training epoch 30, step 12090, learning rate 0.10000000149011612\n",
      "    train loss 0.12869345736809265, train error 0.041866987179487225\n",
      "    test loss 0.49400905761867764, test_error 0.1332000009715557\n",
      "Training epoch 31, step 12480, learning rate 0.10000000149011612\n",
      "    train loss 0.12536014994749656, train error 0.04082532051282051\n",
      "    test loss 0.4800749829038978, test_error 0.13019999861717224\n",
      "Training epoch 32, step 12870, learning rate 0.10000000149011612\n",
      "    train loss 0.11866179258586504, train error 0.03806089743589747\n",
      "    test loss 0.47901271879673, test_error 0.13000000119209287\n",
      "Training epoch 33, step 13260, learning rate 0.10000000149011612\n",
      "    train loss 0.11552049984725622, train error 0.036899038461538414\n",
      "    test loss 0.4661590065807104, test_error 0.1306999988853932\n",
      "Training epoch 34, step 13650, learning rate 0.10000000149011612\n",
      "    train loss 0.11233831462569725, train error 0.035857371794871806\n",
      "    test loss 0.5153129087761045, test_error 0.13389999940991404\n",
      "Training epoch 35, step 14040, learning rate 0.10000000149011612\n",
      "    train loss 0.10566900627544293, train error 0.03279246794871793\n",
      "    test loss 0.44264638517051935, test_error 0.12199999764561653\n",
      "Training epoch 36, step 14430, learning rate 0.10000000149011612\n",
      "    train loss 0.1035247868547837, train error 0.032031249999999956\n",
      "    test loss 0.46675718743354083, test_error 0.12359999939799304\n",
      "Training epoch 37, step 14820, learning rate 0.10000000149011612\n",
      "    train loss 0.09865736839098808, train error 0.03026842948717945\n",
      "    test loss 0.48436033017933366, test_error 0.12829999700188632\n",
      "Training epoch 38, step 15210, learning rate 0.10000000149011612\n",
      "    train loss 0.09753340180390156, train error 0.030408653846153877\n",
      "    test loss 0.4626474123448133, test_error 0.12460000216960909\n",
      "Training epoch 39, step 15600, learning rate 0.10000000149011612\n",
      "    train loss 0.09081844219412559, train error 0.027804487179487136\n",
      "    test loss 0.5084208279848099, test_error 0.12829999849200246\n",
      "Training epoch 40, step 15990, learning rate 0.10000000149011612\n",
      "    train loss 0.0889345952285788, train error 0.027083333333333348\n",
      "    test loss 0.5201408218592405, test_error 0.12619999796152115\n",
      "Training epoch 41, step 16380, learning rate 0.10000000149011612\n",
      "    train loss 0.08605922556553895, train error 0.0262820512820513\n",
      "    test loss 0.4542009778320789, test_error 0.11729999557137494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 42, step 16770, learning rate 0.10000000149011612\n",
      "    train loss 0.0810040378704285, train error 0.024038461538461564\n",
      "    test loss 0.5282047607004643, test_error 0.12599999979138377\n",
      "Training epoch 43, step 17160, learning rate 0.10000000149011612\n",
      "    train loss 0.07791732533906515, train error 0.022816506410256454\n",
      "    test loss 0.4540650319308043, test_error 0.11950000226497648\n",
      "Training epoch 44, step 17550, learning rate 0.10000000149011612\n",
      "    train loss 0.07529216844301957, train error 0.021714743589743568\n",
      "    test loss 0.5157989434897899, test_error 0.12679999843239786\n",
      "Training epoch 45, step 17940, learning rate 0.10000000149011612\n",
      "    train loss 0.071980405283662, train error 0.0213141025641026\n",
      "    test loss 0.5700554966926574, test_error 0.13019999861717224\n",
      "Training epoch 46, step 18330, learning rate 0.10000000149011612\n",
      "    train loss 0.06981048664221397, train error 0.0213141025641026\n",
      "    test loss 0.5305209919810295, test_error 0.12369999960064892\n",
      "Training epoch 47, step 18720, learning rate 0.10000000149011612\n",
      "    train loss 0.06790762869402384, train error 0.01921073717948718\n",
      "    test loss 0.502620654925704, test_error 0.11850000098347668\n",
      "Training epoch 48, step 19110, learning rate 0.10000000149011612\n",
      "    train loss 0.06726087359472727, train error 0.01985176282051282\n",
      "    test loss 0.5261357821524143, test_error 0.12349999845027926\n",
      "Training epoch 49, step 19500, learning rate 0.10000000149011612\n",
      "    train loss 0.06501395667974766, train error 0.01864983974358969\n",
      "    test loss 0.6144723135977983, test_error 0.1335999980568886\n",
      "Training epoch 50, step 19890, learning rate 0.10000000149011612\n",
      "    train loss 0.06261531846024669, train error 0.018169070512820462\n",
      "    test loss 0.5146293831989169, test_error 0.1160000011324882\n",
      "Training epoch 51, step 20280, learning rate 0.10000000149011612\n",
      "    train loss 0.058928468670600496, train error 0.015745192307692335\n",
      "    test loss 0.5066627999767661, test_error 0.12029999941587444\n",
      "Training epoch 52, step 20670, learning rate 0.10000000149011612\n",
      "    train loss 0.057028439316229945, train error 0.0163461538461539\n",
      "    test loss 0.5357397433370352, test_error 0.12700000032782555\n",
      "Training epoch 53, step 21060, learning rate 0.10000000149011612\n",
      "    train loss 0.055090047462055314, train error 0.015184294871794846\n",
      "    test loss 0.5119336893782019, test_error 0.11749999821186063\n",
      "Training epoch 54, step 21450, learning rate 0.10000000149011612\n",
      "    train loss 0.0548103803768754, train error 0.015184294871794846\n",
      "    test loss 0.5404265243560076, test_error 0.12459999769926067\n",
      "Training epoch 55, step 21840, learning rate 0.10000000149011612\n",
      "    train loss 0.05256200103948896, train error 0.014943910256410287\n",
      "    test loss 0.5415588468313217, test_error 0.11989999935030937\n",
      "Training epoch 56, step 22230, learning rate 0.10000000149011612\n",
      "    train loss 0.05148158820871359, train error 0.014362980769230815\n",
      "    test loss 0.5348266439512372, test_error 0.12039999961853032\n",
      "Training epoch 57, step 22620, learning rate 0.10000000149011612\n",
      "    train loss 0.048965799373885, train error 0.013301282051282004\n",
      "    test loss 0.6130872040987014, test_error 0.13069999441504476\n",
      "Training epoch 58, step 23010, learning rate 0.10000000149011612\n",
      "    train loss 0.047421722063938014, train error 0.012840544871794868\n",
      "    test loss 0.5293395025655627, test_error 0.11719999909400936\n",
      "Training epoch 59, step 23400, learning rate 0.10000000149011612\n",
      "    train loss 0.04543055765187511, train error 0.012319711538461564\n",
      "    test loss 0.5451979981735349, test_error 0.12219999954104421\n",
      "Training epoch 60, step 23790, learning rate 0.10000000149011612\n",
      "    train loss 0.04709201919822357, train error 0.012680288461538458\n",
      "    test loss 0.5136455733329057, test_error 0.11549999862909321\n",
      "Training epoch 61, step 24180, learning rate 0.10000000149011612\n",
      "    train loss 0.041569604279282385, train error 0.010957532051282026\n",
      "    test loss 0.5332638887688518, test_error 0.11159999817609789\n",
      "Training epoch 62, step 24570, learning rate 0.10000000149011612\n",
      "    train loss 0.04182582937706358, train error 0.010737179487179449\n",
      "    test loss 0.572445847839117, test_error 0.11989999860525136\n",
      "Training epoch 63, step 24960, learning rate 0.10000000149011612\n",
      "    train loss 0.04132416360199642, train error 0.011338141025641013\n",
      "    test loss 0.5377288185060024, test_error 0.11349999681115153\n",
      "Training epoch 64, step 25350, learning rate 0.10000000149011612\n",
      "    train loss 0.03905288120015309, train error 0.01009615384615381\n",
      "    test loss 0.5584510147571564, test_error 0.11669999957084654\n",
      "Training epoch 65, step 25740, learning rate 0.10000000149011612\n",
      "    train loss 0.036335188389206546, train error 0.009294871794871762\n",
      "    test loss 0.5596590254455804, test_error 0.11569999903440475\n",
      "Training epoch 66, step 26130, learning rate 0.10000000149011612\n",
      "    train loss 0.036717932166245124, train error 0.009094551282051277\n",
      "    test loss 0.5399278316646814, test_error 0.11149999871850014\n",
      "Training epoch 67, step 26520, learning rate 0.10000000149011612\n",
      "    train loss 0.03581643134642106, train error 0.008934294871794868\n",
      "    test loss 0.567212738469243, test_error 0.11469999700784683\n",
      "Training epoch 68, step 26910, learning rate 0.10000000149011612\n",
      "    train loss 0.03761773622618654, train error 0.009675480769230749\n",
      "    test loss 0.580113998427987, test_error 0.11919999793171887\n",
      "Training epoch 69, step 27300, learning rate 0.10000000149011612\n",
      "    train loss 0.03491000076039479, train error 0.008854166666666718\n",
      "    test loss 0.586187051050365, test_error 0.12199999913573267\n",
      "Training epoch 70, step 27690, learning rate 0.10000000149011612\n",
      "    train loss 0.03324319638598424, train error 0.008273237179487136\n",
      "    test loss 0.5451061066240073, test_error 0.10969999581575396\n",
      "Training epoch 71, step 28080, learning rate 0.10000000149011612\n",
      "    train loss 0.03144307524706118, train error 0.0078125\n",
      "    test loss 0.5127725102007389, test_error 0.10869999676942821\n",
      "Training epoch 72, step 28470, learning rate 0.10000000149011612\n",
      "    train loss 0.02884849119716539, train error 0.006790865384615374\n",
      "    test loss 0.5396348785609006, test_error 0.11219999864697461\n",
      "Training epoch 73, step 28860, learning rate 0.10000000149011612\n",
      "    train loss 0.030052816706637925, train error 0.0076322115384614975\n",
      "    test loss 0.6007374677807092, test_error 0.12030000016093256\n",
      "Training epoch 74, step 29250, learning rate 0.10000000149011612\n",
      "    train loss 0.03058709516667594, train error 0.00765224358974359\n",
      "    test loss 0.5345987159758806, test_error 0.10950000062584875\n",
      "Training epoch 75, step 29640, learning rate 0.10000000149011612\n",
      "    train loss 0.029705402700421518, train error 0.007171474358974361\n",
      "    test loss 0.5926426485180855, test_error 0.11529999822378156\n",
      "Training epoch 76, step 30030, learning rate 0.10000000149011612\n",
      "    train loss 0.02888910368275948, train error 0.007111378205128194\n",
      "    test loss 0.5799663655459881, test_error 0.11469999775290485\n",
      "Training epoch 77, step 30420, learning rate 0.10000000149011612\n",
      "    train loss 0.027720196024538613, train error 0.006249999999999978\n",
      "    test loss 0.5801212195307016, test_error 0.11209999769926071\n",
      "Training epoch 78, step 30810, learning rate 0.10000000149011612\n",
      "    train loss 0.02681592842325186, train error 0.006069711538461586\n",
      "    test loss 0.5900506343692541, test_error 0.1120999969542027\n",
      "Training epoch 79, step 31200, learning rate 0.10000000149011612\n",
      "    train loss 0.02500852056635687, train error 0.005889423076923084\n",
      "    test loss 0.5916902607306838, test_error 0.11549999862909321\n",
      "Training epoch 80, step 31590, learning rate 0.10000000149011612\n",
      "    train loss 0.02537915406223291, train error 0.005809294871794823\n",
      "    test loss 0.5810956930741668, test_error 0.10939999967813496\n",
      "Training epoch 81, step 31980, learning rate 0.10000000149011612\n",
      "    train loss 0.024957016648915715, train error 0.00552884615384619\n",
      "    test loss 0.5548824944533408, test_error 0.1119999960064888\n",
      "Training epoch 82, step 32370, learning rate 0.009999999776482582\n",
      "    train loss 0.03153837465275174, train error 0.010516826923076872\n",
      "    test loss 0.4830039070919156, test_error 0.09500000104308126\n",
      "Training epoch 83, step 32760, learning rate 0.009999999776482582\n",
      "    train loss 0.02229137102810618, train error 0.007051282051282026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test loss 0.47644067062065004, test_error 0.09259999915957451\n",
      "Training epoch 84, step 33150, learning rate 0.009999999776482582\n",
      "    train loss 0.018892788938366066, train error 0.006029647435897401\n",
      "    test loss 0.47883996218442915, test_error 0.0916999988257885\n",
      "Training epoch 85, step 33540, learning rate 0.009999999776482582\n",
      "    train loss 0.01753593432561805, train error 0.005588942307692357\n",
      "    test loss 0.48335379268974066, test_error 0.09289999976754193\n",
      "Training epoch 86, step 33930, learning rate 0.009999999776482582\n",
      "    train loss 0.014931423583915696, train error 0.004186698717948745\n",
      "    test loss 0.4812388813123107, test_error 0.09070000201463702\n",
      "Training epoch 87, step 34320, learning rate 0.009999999776482582\n",
      "    train loss 0.014255098827804128, train error 0.004126602564102577\n",
      "    test loss 0.4870315195992589, test_error 0.09150000140070913\n",
      "Training epoch 88, step 34710, learning rate 0.009999999776482582\n",
      "    train loss 0.01417380324117123, train error 0.004186698717948745\n",
      "    test loss 0.48787149479612707, test_error 0.09119999930262568\n",
      "Training epoch 89, step 35100, learning rate 0.009999999776482582\n",
      "    train loss 0.013358282627096065, train error 0.0038461538461538325\n",
      "    test loss 0.49198385048657656, test_error 0.09110000208020208\n",
      "Training epoch 90, step 35490, learning rate 0.009999999776482582\n",
      "    train loss 0.012277466414693313, train error 0.003545673076923106\n",
      "    test loss 0.4909687526524067, test_error 0.09129999950528145\n",
      "Training epoch 91, step 35880, learning rate 0.009999999776482582\n",
      "    train loss 0.011901360393191377, train error 0.003185096153846101\n",
      "    test loss 0.49691548608243463, test_error 0.09149999767541883\n",
      "Training epoch 92, step 36270, learning rate 0.009999999776482582\n",
      "    train loss 0.010915816161865129, train error 0.0028445512820512997\n",
      "    test loss 0.500188066996634, test_error 0.09179999753832813\n",
      "Training epoch 93, step 36660, learning rate 0.009999999776482582\n",
      "    train loss 0.011000088512157209, train error 0.0030849358974358587\n",
      "    test loss 0.4993898294866085, test_error 0.09109999909996991\n",
      "Training epoch 94, step 37050, learning rate 0.009999999776482582\n",
      "    train loss 0.009872680791737273, train error 0.0025040064102563875\n",
      "    test loss 0.5107881464064121, test_error 0.09079999774694447\n",
      "Training epoch 95, step 37440, learning rate 0.009999999776482582\n",
      "    train loss 0.009354286499243851, train error 0.002704326923076872\n",
      "    test loss 0.5115839829668403, test_error 0.0914000026881695\n",
      "Training epoch 96, step 37830, learning rate 0.009999999776482582\n",
      "    train loss 0.010118110138230408, train error 0.002664262820512797\n",
      "    test loss 0.5171410957351327, test_error 0.09249999895691874\n",
      "Training epoch 97, step 38220, learning rate 0.009999999776482582\n",
      "    train loss 0.009373698086710647, train error 0.002804487179487225\n",
      "    test loss 0.5184312131255865, test_error 0.0920999966561794\n",
      "Training epoch 98, step 38610, learning rate 0.009999999776482582\n",
      "    train loss 0.008251728793719592, train error 0.0018028846153845812\n",
      "    test loss 0.5221927164122462, test_error 0.09129999876022343\n",
      "Training epoch 99, step 39000, learning rate 0.009999999776482582\n",
      "    train loss 0.009210585424667583, train error 0.0024839743589744057\n",
      "    test loss 0.5205105599015951, test_error 0.09100000262260433\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session(config=config) as sess:\n",
    "            dataset = Cifar10(train_batch_size)\n",
    "\n",
    "            model = ResNet(num_units, image_shape, train_batch_size, test_batch_size)\n",
    "            train_op, train_loss, train_accuracy = model.build_train_op()\n",
    "            test_loss, test_accuracy = model.build_test_op()\n",
    "\n",
    "            global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "\n",
    "            all_initializer_op = tf.global_variables_initializer()\n",
    "            sess.run(all_initializer_op)\n",
    "\n",
    "            for i in range(epoch):  \n",
    "                total_loss = 0.0\n",
    "                total_accuracy = 0.0\n",
    "\n",
    "                for j in range(dataset.train_batch_count):\n",
    "                    batch_images, batch_labels = dataset.next_aug_train_batch(j)\n",
    "\n",
    "                    sess.run(train_op,\n",
    "                           feed_dict = {model.train_image_placeholder: batch_images, \n",
    "                                        model.train_label_placeholder: batch_labels.reshape(-1)})\n",
    "                    curr_loss, curr_accuracy = sess.run([train_loss,train_accuracy],\n",
    "                                                      feed_dict = {model.train_image_placeholder: batch_images, \n",
    "                                                                   model.train_label_placeholder: batch_labels.reshape(-1)})\n",
    "                    #sess.run(train_step_op)\n",
    "                    total_loss += curr_loss\n",
    "                    total_accuracy += curr_accuracy\n",
    "\n",
    "                total_loss /= dataset.train_batch_count\n",
    "                total_accuracy /= dataset.train_batch_count\n",
    "                print('Training epoch {0}, step {1}, learning rate {2}'.\n",
    "                  format(i, sess.run(model.train_step), sess.run(model.learning_rate)))\n",
    "                print('    train loss {0}, train error {1}'.format(total_loss, 1.0 - total_accuracy))\n",
    "\n",
    "\n",
    "                total_loss = 0.0\n",
    "                total_accuracy = 0.0\n",
    "                for k in range(dataset.test_batch_count):\n",
    "                    batch_images, batch_labels = dataset.next_test_batch(k)\n",
    "\n",
    "                    curr_loss, curr_accuracy = sess.run([test_loss, test_accuracy],\n",
    "                                                      feed_dict = {model.test_image_placeholder: batch_images,\n",
    "                                                                   model.test_label_placeholder: batch_labels.reshape(-1)})\n",
    "                    total_loss += curr_loss\n",
    "                    total_accuracy += curr_accuracy\n",
    "\n",
    "                total_loss /= dataset.test_batch_count\n",
    "                total_accuracy /= dataset.test_batch_count\n",
    "                print('    test loss {0}, test_error {1}'.format(total_loss, 1.0 - total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
